{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONVERT PICKLE MASKS TO TENSOR FORMAT (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_image(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        image_format: str = \"RGB\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Calculates the image embeddings for the provided image, allowing\n",
    "        masks to be predicted with the 'predict' method.\n",
    "\n",
    "        Arguments:\n",
    "          image (np.ndarray): The image for calculating masks. Expects an\n",
    "            image in HWC uint8 format, with pixel values in [0, 255].\n",
    "          image_format (str): The color format of the image, in ['RGB', 'BGR'].\n",
    "        \"\"\"\n",
    "        assert image_format in [\n",
    "            \"RGB\",\n",
    "            \"BGR\",\n",
    "        ], f\"image_format must be in ['RGB', 'BGR'], is {image_format}.\"\n",
    "        if image_format != self.model.image_format:\n",
    "            image = image[..., ::-1]\n",
    "\n",
    "        # Transform the image to the form expected by the model\n",
    "        input_image = self.transform.apply_image(image)\n",
    "        input_image_torch = torch.as_tensor(input_image, device=self.device)\n",
    "        input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "\n",
    "        self.set_torch_image(input_image_torch, image.shape[:2])\n",
    "\n",
    "def set_torch_image(\n",
    "        self,\n",
    "        transformed_image: torch.Tensor,\n",
    "        original_image_size: Tuple[int, ...],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Calculates the image embeddings for the provided image, allowing\n",
    "        masks to be predicted with the 'predict' method. Expects the input\n",
    "        image to be already transformed to the format expected by the model.\n",
    "\n",
    "        Arguments:\n",
    "          transformed_image (torch.Tensor): The input image, with shape\n",
    "            1x3xHxW, which has been transformed with ResizeLongestSide.\n",
    "          original_image_size (tuple(int, int)): The size of the image\n",
    "            before transformation, in (H, W) format.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            len(transformed_image.shape) == 4\n",
    "            and transformed_image.shape[1] == 3\n",
    "            and max(*transformed_image.shape[2:]) == self.model.image_encoder.img_size\n",
    "        ), f\"set_torch_image input must be BCHW with long side {self.model.image_encoder.img_size}.\"\n",
    "        self.reset_image()\n",
    "\n",
    "        self.original_size = original_image_size\n",
    "        self.input_size = tuple(transformed_image.shape[-2:])\n",
    "        input_image = self.model.preprocess(transformed_image)\n",
    "        self.features = self.model.image_encoder(input_image)\n",
    "        self.is_image_set = True\n",
    "\n",
    "def preprocess(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Normalize pixel values and pad to a square input.\"\"\"\n",
    "    # Normalize colors\n",
    "    x = (x - self.pixel_mean) / self.pixel_std\n",
    "\n",
    "    # Pad\n",
    "    h, w = x.shape[-2:]\n",
    "    padh = self.image_encoder.img_size - h\n",
    "    padw = self.image_encoder.img_size - w\n",
    "    x = F.pad(x, (0, padw, 0, padh))\n",
    "    return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
